{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting our data and readying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"task_6_data.txt\").read().split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique answers per group is: \n",
      " *drumroll* \n",
      " 6387 !\n"
     ]
    }
   ],
   "source": [
    "### First we get rid of any weird linebreaks.\n",
    "lines = [line.replace(\"\\n\", \"\") for line in lines]\n",
    "### Now we save all the unique string characters in the lines\n",
    "unique_lines = [''.join(set(line)) for line in lines]\n",
    "### Setting up a counter...\n",
    "line_counter = 0\n",
    "### ...and just counting the number of uniques per group (each element in unique_lines).\n",
    "for unique_line in unique_lines:\n",
    "    line_counter += len(unique_line)\n",
    "print(\"The number of unique answers per group is: \\n *drumroll* \\n\", line_counter, \"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll need to move away from uniques now and find consensus instead: Instances where the entire group answered the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resetting our data\n",
    "lines = open(\"task_6_data.txt\").read().split(\"\\n\\n\")\n",
    "### Now, we'll split the data differently. We're going to make sublists for each group, instead of chunking them together and recording uniques.\n",
    "lines = [line.split(\"\\n\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our democratic counter of consensus has revealed to us that the number of times everyone within the same group answered 'yes' to the same things is \n",
      " *sweetroll... no wait. Wrong roll! Damnit. Ah screw it* \n",
      " 3039\n"
     ]
    }
   ],
   "source": [
    "### We'll make the most democratic counter in the world.\n",
    "consensus_counter = 0\n",
    "\n",
    "#### Now, for each group in the data...\n",
    "for i in range(0, len(lines)):\n",
    "    ### ...Find each unique substring...\n",
    "    for substr in unique_lines[i]:\n",
    "        ## ...count the number of times it occurs in that group's list and sum the total, then compare it to group's size. If these two are equal, we have a match.\n",
    "        if sum([line.count(substr) for line in lines[i]]) == len(lines[i]):\n",
    "            # When we have a match, democracy rules and our consensus_counter goes up by 1. \n",
    "            consensus_counter+=1\n",
    "\n",
    "print(\"Our democratic counter of consensus has revealed to us that the number of times everyone within the same group answered 'yes' to the same things is \\n *sweetroll... no wait. Wrong roll! Damnit. Ah screw it* \\n\", consensus_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
